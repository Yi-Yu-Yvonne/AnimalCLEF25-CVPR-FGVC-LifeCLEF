{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY8Sd1FEl2tf"
      },
      "source": [
        "# AnimalCLEF 2025: ResNet and DINOv2 Implementation\n",
        "\n",
        "This notebook implements both ResNet and DINOv2 models for the AnimalCLEF 2025 competition, which focuses on individual animal identification for three species:\n",
        "- üê¢ Loggerhead sea turtles (Zakynthos, Greece)\n",
        "- ü¶é Salamanders (Czech Republic)\n",
        "- üêÜ Eurasian lynxes (Czech Republic)\n",
        "\n",
        "The goal is to determine whether an animal in an image is new (not present in the training dataset) or known (in which case, its identity must be provided)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRvHWh8ml2tg"
      },
      "source": [
        "## 1. Environment Setup and Dependencies Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCH_NUM = 5 # 15, 30\n",
        "FINETUNE_RESNET = True\n",
        "FINETUNE_DINO = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "LnNk_e6epFol"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image, ImageFilter\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModel, AutoImageProcessor\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBI4pLmFpN3J",
        "outputId": "348523d4-cb32-416d-ac06-ea0af2708c31"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q git+https://github.com/WildlifeDatasets/wildlife-datasets@develop\n",
        "!pip install -q git+https://github.com/WildlifeDatasets/wildlife-tools\n",
        "!pip install -q timm transformers albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ly8iMYuppl1",
        "outputId": "52e7de03-7475-4c9e-b310-c235ada3f6c8"
      },
      "outputs": [],
      "source": [
        "# Set data path\n",
        "DATA_PATH = 'animal-clef-2025'\n",
        "# Create directory for saving models\n",
        "MODEL_SAVE_PATH = '/home/intern/Yu/animal-clef-2025/training'\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "VNPU6eG6l2th",
        "outputId": "645e152e-b536-484b-bbf2-31775b8c8902"
      },
      "outputs": [],
      "source": [
        "# Check if running in Kaggle or Colab\n",
        "IN_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_KAGGLE:\n",
        "    print(\"Running in Kaggle\")\n",
        "    # Set data path\n",
        "    DATA_PATH = '/kaggle/input/animal-clef-2025'\n",
        "    MODEL_SAVE_PATH = '/kaggle/working/models'\n",
        "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "elif IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "\n",
        "    # Upload kaggle.json if needed\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        files.upload()  # Upload kaggle.json\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !cp kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "    except:\n",
        "        print(\"Please upload your kaggle.json file to access competition data\")\n",
        "\n",
        "    # Download competition data\n",
        "    !kaggle competitions download -c animal-clef-2025\n",
        "    !unzip -q animal-clef-2025.zip -d animal-clef-2025\n",
        "\n",
        "else:\n",
        "    print(\"Running in local environment\")\n",
        "    # Set appropriate path for your environment\n",
        "    DATA_PATH = './animal-clef-2025'\n",
        "    MODEL_SAVE_PATH = './models'\n",
        "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zK_hgoopJVi",
        "outputId": "b2cd264e-f704-493e-d508-8041ccbccebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1zIKCRhl2th"
      },
      "source": [
        "## 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "gLaWlMF-l2th",
        "outputId": "faa280a8-6d7c-41fb-ff7f-497abce35166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata shape: (15209, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>identity</th>\n",
              "      <th>path</th>\n",
              "      <th>date</th>\n",
              "      <th>orientation</th>\n",
              "      <th>species</th>\n",
              "      <th>split</th>\n",
              "      <th>dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>LynxID2025_lynx_37</td>\n",
              "      <td>images/LynxID2025/database/000f9ee1aad063a4485...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>right</td>\n",
              "      <td>lynx</td>\n",
              "      <td>database</td>\n",
              "      <td>LynxID2025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>LynxID2025_lynx_37</td>\n",
              "      <td>images/LynxID2025/database/0020edb6689e9f78462...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>left</td>\n",
              "      <td>lynx</td>\n",
              "      <td>database</td>\n",
              "      <td>LynxID2025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>LynxID2025_lynx_49</td>\n",
              "      <td>images/LynxID2025/database/003152e4145b5b69400...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>left</td>\n",
              "      <td>lynx</td>\n",
              "      <td>database</td>\n",
              "      <td>LynxID2025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>images/LynxID2025/query/003b89301c7b9f6d18f722...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>back</td>\n",
              "      <td>lynx</td>\n",
              "      <td>query</td>\n",
              "      <td>LynxID2025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>LynxID2025_lynx_13</td>\n",
              "      <td>images/LynxID2025/database/003c3f82011e9c3f849...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>right</td>\n",
              "      <td>lynx</td>\n",
              "      <td>database</td>\n",
              "      <td>LynxID2025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id            identity  \\\n",
              "0         0  LynxID2025_lynx_37   \n",
              "1         1  LynxID2025_lynx_37   \n",
              "2         2  LynxID2025_lynx_49   \n",
              "3         3                 NaN   \n",
              "4         4  LynxID2025_lynx_13   \n",
              "\n",
              "                                                path date orientation species  \\\n",
              "0  images/LynxID2025/database/000f9ee1aad063a4485...  NaN       right    lynx   \n",
              "1  images/LynxID2025/database/0020edb6689e9f78462...  NaN        left    lynx   \n",
              "2  images/LynxID2025/database/003152e4145b5b69400...  NaN        left    lynx   \n",
              "3  images/LynxID2025/query/003b89301c7b9f6d18f722...  NaN        back    lynx   \n",
              "4  images/LynxID2025/database/003c3f82011e9c3f849...  NaN       right    lynx   \n",
              "\n",
              "      split     dataset  \n",
              "0  database  LynxID2025  \n",
              "1  database  LynxID2025  \n",
              "2  database  LynxID2025  \n",
              "3     query  LynxID2025  \n",
              "4  database  LynxID2025  "
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load metadata\n",
        "metadata_path = os.path.join(DATA_PATH, 'metadata.csv')\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "\n",
        "# Display basic information about the metadata\n",
        "print(f\"Metadata shape: {metadata.shape}\")\n",
        "metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcZJgigRl2th",
        "outputId": "cc18380a-2291-4192-9c52-7663a100d3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in each column:\n",
            "image_id          0\n",
            "identity       2135\n",
            "path              0\n",
            "date           3907\n",
            "orientation     703\n",
            "species        1388\n",
            "split             0\n",
            "dataset           0\n",
            "dtype: int64\n",
            "\n",
            "Unique values in 'species' column:\n",
            "['lynx' nan 'salamander' 'loggerhead turtle']\n",
            "\n",
            "Unique values in 'orientation' column:\n",
            "['right' 'left' 'back' 'front' 'unknown' 'top' 'bottom' 'topright'\n",
            " 'topleft' nan 'down']\n",
            "\n",
            "Unique values in 'dataset' column:\n",
            "['LynxID2025' 'SalamanderID2025' 'SeaTurtleID2022']\n",
            "\n",
            "Unique values in 'split' column:\n",
            "['database' 'query']\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values in the metadata\n",
        "missing_values = metadata.isnull().sum()\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Check unique values in categorical columns\n",
        "print(\"\\nUnique values in 'species' column:\")\n",
        "print(metadata['species'].unique())\n",
        "\n",
        "print(\"\\nUnique values in 'orientation' column:\")\n",
        "print(metadata['orientation'].unique())\n",
        "\n",
        "print(\"\\nUnique values in 'dataset' column:\")\n",
        "print(metadata['dataset'].unique())\n",
        "\n",
        "print(\"\\nUnique values in 'split' column:\")\n",
        "print(metadata['split'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tnSjc0Vl2ti"
      },
      "source": [
        "### 2.1 Handling NaN Values in Metadata\n",
        "\n",
        "Based on the analysis above, we need to handle several types of missing values in the metadata:\n",
        "\n",
        "1. **Empty identity fields for query images**: This is expected since these are the images we need to identify.\n",
        "2. **Missing date values**: We'll fill these with a default value since the date might not be critical for our model.\n",
        "3. **Unknown orientation values**: We'll treat 'unknown' as a separate category in our encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIJSa6vol2ti",
        "outputId": "cc555b0f-beba-4283-ad5a-b5ebb657d708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query images with missing identity: 2135 out of 2135\n",
            "Database images with identity: 13074 out of 13074\n",
            "\n",
            "Orientation distribution:\n",
            "orientation\n",
            "right       4655\n",
            "left        4231\n",
            "top         2009\n",
            "topright    1490\n",
            "topleft     1354\n",
            "NaN          703\n",
            "front        318\n",
            "unknown      245\n",
            "back         167\n",
            "down          34\n",
            "bottom         3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Missing values after processing:\n",
            "image_id          0\n",
            "identity       2135\n",
            "path              0\n",
            "date              0\n",
            "orientation       0\n",
            "species        1388\n",
            "split             0\n",
            "dataset           0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create a copy of the metadata for preprocessing\n",
        "metadata_processed = metadata.copy()\n",
        "\n",
        "# 1. Handle missing identity values for query images\n",
        "# For query images, we'll keep the identity as NaN since these are what we need to predict\n",
        "# We'll verify that all query images have NaN identity and all database images have an identity\n",
        "query_missing_identity = metadata_processed[metadata_processed['split'] == 'query']['identity'].isnull().sum()\n",
        "query_total = metadata_processed[metadata_processed['split'] == 'query'].shape[0]\n",
        "database_has_identity = metadata_processed[metadata_processed['split'] == 'database']['identity'].notnull().sum()\n",
        "database_total = metadata_processed[metadata_processed['split'] == 'database'].shape[0]\n",
        "\n",
        "print(f\"Query images with missing identity: {query_missing_identity} out of {query_total}\")\n",
        "print(f\"Database images with identity: {database_has_identity} out of {database_total}\")\n",
        "\n",
        "# 2. Handle missing date values\n",
        "# Fill missing dates with a default value 'unknown_date'\n",
        "metadata_processed['date'] = metadata_processed['date'].fillna('unknown_date')\n",
        "\n",
        "# 3. Handle orientation values\n",
        "# First, check the distribution of orientation values\n",
        "orientation_counts = metadata_processed['orientation'].value_counts(dropna=False)\n",
        "print(\"\\nOrientation distribution:\")\n",
        "print(orientation_counts)\n",
        "\n",
        "# Fill missing orientation values with 'unknown'\n",
        "metadata_processed['orientation'] = metadata_processed['orientation'].fillna('unknown')\n",
        "\n",
        "# Verify that we've handled all missing values except for identity in query images\n",
        "missing_after = metadata_processed.isnull().sum()\n",
        "print(\"\\nMissing values after processing:\")\n",
        "print(missing_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JxnhdU4l2ti"
      },
      "source": [
        "### 2.2 Encoding Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "t6dMe0sMl2ti",
        "outputId": "48819095-75d6-4ed1-8545-ae9f3eab3f1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed metadata:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>identity</th>\n",
              "      <th>path</th>\n",
              "      <th>date</th>\n",
              "      <th>orientation</th>\n",
              "      <th>species</th>\n",
              "      <th>split</th>\n",
              "      <th>dataset</th>\n",
              "      <th>species_encoded</th>\n",
              "      <th>orientation_encoded</th>\n",
              "      <th>dataset_encoded</th>\n",
              "      <th>identity_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>LynxID2025_lynx_37</td>\n",
              "      <td>images/LynxID2025/database/000f9ee1aad063a4485...</td>\n",
              "      <td>unknown_date</td>\n",
              "      <td>right</td>\n",
              "      <td>lynx</td>\n",
              "      <td>database</td>\n",
              "      <td>LynxID2025</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>LynxID2025_lynx_37</td>\n",
              "      <td>images/LynxID2025/database/0020edb6689e9f78462...</td>\n",
              "      <td>unknown_date</td>\n",
              "      <td>left</td>\n",
              "      <td>lynx</td>\n",
              "      <td>database</td>\n",
              "      <td>LynxID2025</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>LynxID2025_lynx_49</td>\n",
              "      <td>images/LynxID2025/database/003152e4145b5b69400...</td>\n",
              "      <td>unknown_date</td>\n",
              "      <td>left</td>\n",
              "      <td>lynx</td>\n",
              "      <td>database</td>\n",
              "      <td>LynxID2025</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>images/LynxID2025/query/003b89301c7b9f6d18f722...</td>\n",
              "      <td>unknown_date</td>\n",
              "      <td>back</td>\n",
              "      <td>lynx</td>\n",
              "      <td>query</td>\n",
              "      <td>LynxID2025</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>LynxID2025_lynx_13</td>\n",
              "      <td>images/LynxID2025/database/003c3f82011e9c3f849...</td>\n",
              "      <td>unknown_date</td>\n",
              "      <td>right</td>\n",
              "      <td>lynx</td>\n",
              "      <td>database</td>\n",
              "      <td>LynxID2025</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id            identity  \\\n",
              "0         0  LynxID2025_lynx_37   \n",
              "1         1  LynxID2025_lynx_37   \n",
              "2         2  LynxID2025_lynx_49   \n",
              "3         3                 NaN   \n",
              "4         4  LynxID2025_lynx_13   \n",
              "\n",
              "                                                path          date  \\\n",
              "0  images/LynxID2025/database/000f9ee1aad063a4485...  unknown_date   \n",
              "1  images/LynxID2025/database/0020edb6689e9f78462...  unknown_date   \n",
              "2  images/LynxID2025/database/003152e4145b5b69400...  unknown_date   \n",
              "3  images/LynxID2025/query/003b89301c7b9f6d18f722...  unknown_date   \n",
              "4  images/LynxID2025/database/003c3f82011e9c3f849...  unknown_date   \n",
              "\n",
              "  orientation species     split     dataset  species_encoded  \\\n",
              "0       right    lynx  database  LynxID2025                1   \n",
              "1        left    lynx  database  LynxID2025                1   \n",
              "2        left    lynx  database  LynxID2025                1   \n",
              "3        back    lynx     query  LynxID2025                1   \n",
              "4       right    lynx  database  LynxID2025                1   \n",
              "\n",
              "   orientation_encoded  dataset_encoded  identity_encoded  \n",
              "0                    5                0              29.0  \n",
              "1                    4                0              29.0  \n",
              "2                    4                0              40.0  \n",
              "3                    0                0               NaN  \n",
              "4                    5                0              11.0  "
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encode categorical features\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode species\n",
        "species_encoder = LabelEncoder()\n",
        "metadata_processed['species_encoded'] = species_encoder.fit_transform(metadata_processed['species'])\n",
        "\n",
        "# Encode orientation\n",
        "orientation_encoder = LabelEncoder()\n",
        "metadata_processed['orientation_encoded'] = orientation_encoder.fit_transform(metadata_processed['orientation'])\n",
        "\n",
        "# Encode dataset\n",
        "dataset_encoder = LabelEncoder()\n",
        "metadata_processed['dataset_encoded'] = dataset_encoder.fit_transform(metadata_processed['dataset'])\n",
        "\n",
        "# For identity, we'll create a mapping for database images only\n",
        "# Query images will be handled separately during prediction\n",
        "database_identities = metadata_processed[metadata_processed['split'] == 'database']['identity'].dropna().unique()\n",
        "identity_encoder = LabelEncoder()\n",
        "identity_encoder.fit(list(database_identities) + ['new_individual'])  # Add 'new_individual' as a class\n",
        "\n",
        "# Create a new column for encoded identity, but only fill it for database images\n",
        "metadata_processed['identity_encoded'] = np.nan\n",
        "mask = metadata_processed['split'] == 'database'\n",
        "metadata_processed.loc[mask, 'identity_encoded'] = identity_encoder.transform(metadata_processed.loc[mask, 'identity'])\n",
        "\n",
        "# Display the processed metadata\n",
        "print(\"Processed metadata:\")\n",
        "metadata_processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwA9znNSl2ti"
      },
      "source": [
        "### 2.3 Creating Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9boJXzFll2ti",
        "outputId": "3afe5f29-dd5f-4a5d-ceb7-b075c9a2b035"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/intern/Yu/Animal-Clef-2025/animalvenv/lib/python3.11/site-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Define image transformations for ResNet\n",
        "resnet_train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "resnet_val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define image transformations for DINOv2 using Albumentations\n",
        "dinov2_train_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.RandomCrop(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "dinov2_val_transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Create dataset class for ResNet\n",
        "class AnimalDatasetResNet(Dataset):\n",
        "    def __init__(self, metadata, data_path, transform=None, is_query=False):\n",
        "        self.metadata = metadata\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.is_query = is_query\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata.iloc[idx]\n",
        "        img_path = os.path.join(self.data_path, row['path'])\n",
        "\n",
        "        # Load and transform image\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a blank image if there's an error\n",
        "            img = torch.zeros((3, 224, 224))\n",
        "\n",
        "        # For query images, we only return the image and metadata\n",
        "        if self.is_query:\n",
        "            return {\n",
        "                'image': img,\n",
        "                'image_id': row['image_id'],\n",
        "                'path': row['path'],\n",
        "                'species': row['species_encoded'],\n",
        "                'orientation': row['orientation_encoded'],\n",
        "                'dataset': row['dataset_encoded']\n",
        "            }\n",
        "\n",
        "        # For database images, we also return the identity\n",
        "        return {\n",
        "            'image': img,\n",
        "            'identity': row['identity_encoded'],\n",
        "            'image_id': row['image_id'],\n",
        "            'path': row['path'],\n",
        "            'species': row['species_encoded'],\n",
        "            'orientation': row['orientation_encoded'],\n",
        "            'dataset': row['dataset_encoded']\n",
        "        }\n",
        "\n",
        "# Create dataset class for DINOv2\n",
        "class AnimalDatasetDINOv2(Dataset):\n",
        "    def __init__(self, metadata, data_path, transform=None, is_query=False):\n",
        "        self.metadata = metadata\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.is_query = is_query\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata.iloc[idx]\n",
        "        img_path = os.path.join(self.data_path, row['path'])\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            img = np.array(Image.open(img_path).convert('RGB'))\n",
        "            if self.transform:\n",
        "                transformed = self.transform(image=img)\n",
        "                img = transformed['image']\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a blank image if there's an error\n",
        "            img = torch.zeros((3, 224, 224))\n",
        "\n",
        "        # For query images, we only return the image and metadata\n",
        "        if self.is_query:\n",
        "            return {\n",
        "                'image': img,\n",
        "                'image_id': row['image_id'],\n",
        "                'path': row['path'],\n",
        "                'species': row['species_encoded'],\n",
        "                'orientation': row['orientation_encoded'],\n",
        "                'dataset': row['dataset_encoded']\n",
        "            }\n",
        "\n",
        "        # For database images, we also return the identity\n",
        "        return {\n",
        "            'image': img,\n",
        "            'identity': row['identity_encoded'],\n",
        "            'image_id': row['image_id'],\n",
        "            'path': row['path'],\n",
        "            'species': row['species_encoded'],\n",
        "            'orientation': row['orientation_encoded'],\n",
        "            'dataset': row['dataset_encoded']\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGkAl4D-l2tj"
      },
      "source": [
        "### 2.4 Preparing Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXVxd2PVl2tj",
        "outputId": "7842d0a9-2ec7-4606-8d08-6d596c0bf89d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of identities: 1102\n",
            "Identities with only one sample: 317\n",
            "Data with multiple samples per identity: 12757\n",
            "Data with single sample per identity: 317\n",
            "Training data size: 10522\n",
            "Validation data size: 2552\n",
            "Query data size: 2135\n"
          ]
        }
      ],
      "source": [
        "# Split database data into train and validation sets\n",
        "database_data = metadata_processed[metadata_processed['split'] == 'database']\n",
        "query_data = metadata_processed[metadata_processed['split'] == 'query']\n",
        "\n",
        "# Check identity distribution to identify classes with only one sample\n",
        "identity_counts = database_data['identity'].value_counts()\n",
        "print(f\"Number of identities: {len(identity_counts)}\")\n",
        "print(f\"Identities with only one sample: {sum(identity_counts == 1)}\")\n",
        "\n",
        "# Filter out identities with only one sample for stratification\n",
        "# These will be added to the training set directly\n",
        "single_sample_identities = identity_counts[identity_counts == 1].index.tolist()\n",
        "multi_sample_data = database_data[~database_data['identity'].isin(single_sample_identities)]\n",
        "single_sample_data = database_data[database_data['identity'].isin(single_sample_identities)]\n",
        "\n",
        "print(f\"Data with multiple samples per identity: {len(multi_sample_data)}\")\n",
        "print(f\"Data with single sample per identity: {len(single_sample_data)}\")\n",
        "\n",
        "# Stratify only the data with multiple samples per identity\n",
        "if len(multi_sample_data) > 0:\n",
        "    train_data, val_data = train_test_split(\n",
        "        multi_sample_data,\n",
        "        test_size=0.2,\n",
        "        random_state=SEED,\n",
        "        stratify=multi_sample_data['identity']\n",
        "    )\n",
        "\n",
        "    # Add single sample data to training set\n",
        "    train_data = pd.concat([train_data, single_sample_data])\n",
        "else:\n",
        "    # If there's no multi-sample data, use a simple random split\n",
        "    train_data, val_data = train_test_split(\n",
        "        database_data,\n",
        "        test_size=0.2,\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "print(f\"Training data size: {len(train_data)}\")\n",
        "print(f\"Validation data size: {len(val_data)}\")\n",
        "print(f\"Query data size: {len(query_data)}\")\n",
        "\n",
        "# Create datasets for ResNet\n",
        "train_dataset_resnet = AnimalDatasetResNet(\n",
        "    train_data,\n",
        "    DATA_PATH,\n",
        "    transform=resnet_train_transform,\n",
        "    is_query=False\n",
        ")\n",
        "\n",
        "val_dataset_resnet = AnimalDatasetResNet(\n",
        "    val_data,\n",
        "    DATA_PATH,\n",
        "    transform=resnet_val_transform,\n",
        "    is_query=False\n",
        ")\n",
        "\n",
        "query_dataset_resnet = AnimalDatasetResNet(\n",
        "    query_data,\n",
        "    DATA_PATH,\n",
        "    transform=resnet_val_transform,\n",
        "    is_query=True\n",
        ")\n",
        "\n",
        "# Create datasets for DINOv2\n",
        "train_dataset_dinov2 = AnimalDatasetDINOv2(\n",
        "    train_data,\n",
        "    DATA_PATH,\n",
        "    transform=dinov2_train_transform,\n",
        "    is_query=False\n",
        ")\n",
        "\n",
        "val_dataset_dinov2 = AnimalDatasetDINOv2(\n",
        "    val_data,\n",
        "    DATA_PATH,\n",
        "    transform=dinov2_val_transform,\n",
        "    is_query=False\n",
        ")\n",
        "\n",
        "query_dataset_dinov2 = AnimalDatasetDINOv2(\n",
        "    query_data,\n",
        "    DATA_PATH,\n",
        "    transform=dinov2_val_transform,\n",
        "    is_query=True\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "train_loader_resnet = DataLoader(\n",
        "    train_dataset_resnet,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader_resnet = DataLoader(\n",
        "    val_dataset_resnet,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "query_loader_resnet = DataLoader(\n",
        "    query_dataset_resnet,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "train_loader_dinov2 = DataLoader(\n",
        "    train_dataset_dinov2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader_dinov2 = DataLoader(\n",
        "    val_dataset_dinov2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "query_loader_dinov2 = DataLoader(\n",
        "    query_dataset_dinov2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo_aDbikl2tj"
      },
      "source": [
        "## 3. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jLeEOodl2tj"
      },
      "source": [
        "### 3.1 ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Pznrxf-fl2tj"
      },
      "outputs": [],
      "source": [
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_dim=512):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        # Load pre-trained ResNet50\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "        in_features = self.backbone.fc.in_features\n",
        "\n",
        "        # Replace the final fully connected layer\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # Add embedding layer\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Linear(in_features, embedding_dim),\n",
        "            nn.BatchNorm1d(embedding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Add classifier head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, return_embeddings=False):\n",
        "        # Extract features from backbone\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Get embeddings\n",
        "        embeddings = self.embedding(features)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return embeddings\n",
        "\n",
        "        # Get class predictions\n",
        "        logits = self.classifier(embeddings)\n",
        "\n",
        "        return logits, embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcC1sa1Pl2tj"
      },
      "source": [
        "### 3.2 DINOv2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "y5LrLD8ul2tj"
      },
      "outputs": [],
      "source": [
        "class DINOv2Model(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_dim=512):\n",
        "        super(DINOv2Model, self).__init__()\n",
        "        # Load pre-trained DINOv2 model\n",
        "        self.processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "\n",
        "        # Get the output dimension of the backbone\n",
        "        in_features = self.backbone.config.hidden_size\n",
        "\n",
        "        # Add embedding layer\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Linear(in_features, embedding_dim),\n",
        "            nn.BatchNorm1d(embedding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Add classifier head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, return_embeddings=False):\n",
        "        # Extract features from backbone\n",
        "        outputs = self.backbone(x)\n",
        "        features = outputs.last_hidden_state[:, 0]  # Use CLS token\n",
        "\n",
        "        # Get embeddings\n",
        "        embeddings = self.embedding(features)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return embeddings\n",
        "\n",
        "        # Get class predictions\n",
        "        logits = self.classifier(embeddings)\n",
        "\n",
        "        return logits, embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYbdRHlJl2tj"
      },
      "source": [
        "### 3.3 Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "hOJ1PNibl2tk"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, dataloader, num_epochs=10, model_name=\"model\"):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data\n",
        "        for batch in tqdm(dataloader, desc=f\"Training {model_name}\"):\n",
        "            inputs = batch['image'].to(device)\n",
        "            labels = batch['identity'].long().to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.set_grad_enabled(True):\n",
        "                outputs, embeddings = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc.item())\n",
        "\n",
        "        # Save the best model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), os.path.join(MODEL_SAVE_PATH, f\"{model_name}_best.pth\"))\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': epoch_loss,\n",
        "            }, os.path.join(MODEL_SAVE_PATH, f\"{model_name}_epoch_{epoch+1}.pth\"))\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA5UKb0ql2tk"
      },
      "source": [
        "### 3.4 Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Hj8rnsXFl2tk"
      },
      "outputs": [],
      "source": [
        "def extract_embeddings(model, dataloader, model_name):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    image_ids = []\n",
        "    paths = []\n",
        "    identities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=f\"Extracting embeddings with {model_name}\"):\n",
        "            inputs = batch['image'].to(device)\n",
        "\n",
        "            # Get embeddings\n",
        "            if model_name == 'resnet':\n",
        "                batch_embeddings = model(inputs, return_embeddings=True)\n",
        "            else:  # dinov2\n",
        "                batch_embeddings = model(inputs, return_embeddings=True)\n",
        "\n",
        "            # Move to CPU and convert to numpy\n",
        "            batch_embeddings = batch_embeddings.cpu().numpy()\n",
        "\n",
        "            # Store embeddings and metadata\n",
        "            embeddings.append(batch_embeddings)\n",
        "            image_ids.extend(batch['image_id'].numpy())\n",
        "            paths.extend(batch['path'])\n",
        "\n",
        "            # Store identities if available (for database images)\n",
        "            if 'identity' in batch:\n",
        "                identities.extend(batch['identity'].numpy())\n",
        "\n",
        "    # Concatenate embeddings\n",
        "    embeddings = np.vstack(embeddings)\n",
        "\n",
        "    # Create a dictionary with all information\n",
        "    result = {\n",
        "        'embeddings': embeddings,\n",
        "        'image_ids': np.array(image_ids),\n",
        "        'paths': np.array(paths)\n",
        "    }\n",
        "\n",
        "    if identities:\n",
        "        result['identities'] = np.array(identities)\n",
        "\n",
        "    return result\n",
        "\n",
        "def evaluate_model(database_embeddings, query_embeddings, identity_encoder, threshold=0.5):\n",
        "    # Normalize embeddings\n",
        "    database_norm = database_embeddings['embeddings'] / np.linalg.norm(database_embeddings['embeddings'], axis=1, keepdims=True)\n",
        "    query_norm = query_embeddings['embeddings'] / np.linalg.norm(query_embeddings['embeddings'], axis=1, keepdims=True)\n",
        "\n",
        "    # Find nearest neighbors\n",
        "    knn = NearestNeighbors(n_neighbors=1, metric='cosine')\n",
        "    knn.fit(database_norm)\n",
        "    distances, indices = knn.kneighbors(query_norm)\n",
        "\n",
        "    # Convert distances to similarities (1 - distance)\n",
        "    similarities = 1 - distances\n",
        "\n",
        "    # Predict identities\n",
        "    predicted_identities = []\n",
        "    for i, (similarity, idx) in enumerate(zip(similarities.flatten(), indices.flatten())):\n",
        "        if similarity >= threshold:\n",
        "            # Known individual\n",
        "            predicted_identities.append(database_embeddings['identities'][idx])\n",
        "        else:\n",
        "            # New individual\n",
        "            predicted_identities.append(identity_encoder.transform(['new_individual'])[0])\n",
        "\n",
        "    return np.array(predicted_identities), similarities.flatten()\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    # Calculate standard metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    # Calculate BAKS (Balanced Accuracy for Known Samples)\n",
        "    # This is the accuracy for samples that are known individuals\n",
        "    known_mask = y_true != identity_encoder.transform(['new_individual'])[0]\n",
        "    if np.sum(known_mask) > 0:\n",
        "        baks = accuracy_score(y_true[known_mask], y_pred[known_mask])\n",
        "    else:\n",
        "        baks = 0.0\n",
        "\n",
        "    # Calculate BAUS (Balanced Accuracy for Unknown Samples)\n",
        "    # This is the accuracy for samples that are new individuals\n",
        "    unknown_mask = y_true == identity_encoder.transform(['new_individual'])[0]\n",
        "    if np.sum(unknown_mask) > 0:\n",
        "        baus = accuracy_score(y_true[unknown_mask], y_pred[unknown_mask])\n",
        "    else:\n",
        "        baus = 0.0\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'baks': baks,\n",
        "        'baus': baus\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVGvjAEl2tk"
      },
      "source": [
        "## 4. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "TfNVzxWAtIuR"
      },
      "outputs": [],
      "source": [
        "# Upload RestNet model and history manually or use this to upload from local\n",
        "#  from google.colab import files\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.serialization\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not FINETUNE_RESNET:\n",
        "    num_classes = len(identity_encoder.classes_)\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Initialize ResNet model\n",
        "    resnet_model = ResNetModel(num_classes=num_classes, embedding_dim=512).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "FLK9rSV-e-MW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/intern/Yu/Animal-Clef-2025/animalvenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/intern/Yu/Animal-Clef-2025/animalvenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "if FINETUNE_RESNET: \n",
        "    # uploaded = files.upload()  # Upload resnet_model.pth\n",
        "    \n",
        "    # Allowlist LabelEncoder\n",
        "    torch.serialization.add_safe_globals({'LabelEncoder': LabelEncoder})\n",
        "\n",
        "    # Now load checkpoint\n",
        "    checkpoint = torch.load(f'resnet_model_history/resnet_model_{EPOCH_NUM}.pth', map_location=device, weights_only=False)\n",
        "\n",
        "    # Extract encoder\n",
        "    identity_encoder = checkpoint['identity_encoder']\n",
        "\n",
        "    # Initialize model with correct number of classes\n",
        "    num_classes = len(identity_encoder.classes_)\n",
        "    resnet_model = ResNetModel(num_classes=num_classes, embedding_dim=512).to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    resnet_optimizer = optim.Adam(resnet_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    resnet_scheduler = optim.lr_scheduler.StepLR(resnet_optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # Load model weights and training states\n",
        "    resnet_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    resnet_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    resnet_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    # # Load later\n",
        "    # with open(f'resnet_model_history/resnet_history_{EPOCH_NUM}.pth', 'rb') as f:\n",
        "    #     resnet_history = pickle.load(f)\n",
        "\n",
        "    # print(\"‚úÖ ResNet model and optimizer state loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "GuYrt4pTl2tk"
      },
      "outputs": [],
      "source": [
        "# # Get number of classes (identities)\n",
        "# num_classes = len(identity_encoder.classes_)\n",
        "# print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# # Initialize ResNet model\n",
        "# resnet_model = ResNetModel(num_classes=num_classes, embedding_dim=512).to(device)\n",
        "\n",
        "# # Initialize optimizer and scheduler for ResNet\n",
        "# resnet_optimizer = optim.Adam(resnet_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "# resnet_scheduler = optim.lr_scheduler.StepLR(resnet_optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# # Initialize loss function\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Train ResNet model\n",
        "# print(\"Training ResNet model...\")\n",
        "# resnet_model, resnet_history = train_model(\n",
        "#     resnet_model,\n",
        "#     criterion,\n",
        "#     resnet_optimizer,\n",
        "#     resnet_scheduler,\n",
        "#     train_loader_resnet,\n",
        "#     num_epochs=5,\n",
        "#     model_name=\"resnet\"\n",
        "# )\n",
        "\n",
        "# torch.save({\n",
        "#     'model_state_dict': resnet_model.state_dict(),\n",
        "#     'optimizer_state_dict': resnet_optimizer.state_dict(),\n",
        "#     'scheduler_state_dict': resnet_scheduler.state_dict(),\n",
        "#     'identity_encoder': identity_encoder  # if picklable\n",
        "# }, 'resnet_model.pth')\n",
        "\n",
        "# print(\"Model saved locally in Colab environment.\")\n",
        "\n",
        "# # Save history\n",
        "# with open('resnet_history.pkl', 'wb') as f:\n",
        "#     pickle.dump(resnet_history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not FINETUNE_DINO:\n",
        "    dinov2_model = DINOv2Model(num_classes=num_classes, embedding_dim=512).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-F8vimpPfKWd",
        "outputId": "5e59a112-dba2-4a21-f95f-2d75c3492bda"
      },
      "outputs": [],
      "source": [
        "# Upload DINOv2 model and history manually or use this to upload from local\n",
        "if FINETUNE_DINO:\n",
        "    # uploaded = files.upload()  # Upload resnet_model.pth\n",
        "\n",
        "    # Allowlist LabelEncoder\n",
        "    torch.serialization.add_safe_globals({'LabelEncoder': LabelEncoder})\n",
        "\n",
        "    # Now load checkpoint\n",
        "    checkpoint = torch.load(f'dino_model_history/dinov2_history_{EPOCH_NUM}.pth', map_location=device, weights_only=False)\n",
        "\n",
        "    # Extract encoder\n",
        "    identity_encoder = checkpoint['identity_encoder']\n",
        "\n",
        "    # Initialize model with correct number of classes\n",
        "    num_classes = len(identity_encoder.classes_)\n",
        "    dinov2_model = DINOv2Model(num_classes=num_classes, embedding_dim=512).to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    dinov2_optimizer = optim.Adam(dinov2_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    dinov2_scheduler = optim.lr_scheduler.StepLR(dinov2_optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # Load model weights and training states\n",
        "    dinov2_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    dinov2_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    dinov2_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    # Load later\n",
        "    # with open(f'dino_model_history/dinov2_history_{EPOCH_NUM}.pkl', 'rb') as f:\n",
        "    #     dinov2_history = pickle.load(f)\n",
        "\n",
        "    # print(\"‚úÖ DINOv2 model and optimizer state loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "M19qHtgdl2tk"
      },
      "outputs": [],
      "source": [
        "# # Initialize DINOv2 model\n",
        "# dinov2_model = DINOv2Model(num_classes=num_classes, embedding_dim=512).to(device)\n",
        "\n",
        "# # Initialize optimizer and scheduler for DINOv2\n",
        "# dinov2_optimizer = optim.Adam(dinov2_model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
        "# dinov2_scheduler = optim.lr_scheduler.StepLR(dinov2_optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# # Train DINOv2 model\n",
        "# print(\"Training DINOv2 model...\")\n",
        "# dinov2_model, dinov2_history = train_model(\n",
        "#     dinov2_model,\n",
        "#     criterion,\n",
        "#     dinov2_optimizer,\n",
        "#     dinov2_scheduler,\n",
        "#     train_loader_dinov2,\n",
        "#     num_epochs=5,\n",
        "#     model_name=\"dinov2\"\n",
        "# )\n",
        "\n",
        "# # Save model state_dict\n",
        "# torch.save({\n",
        "#     'model_state_dict': dinov2_model.state_dict(),\n",
        "#     'optimizer_state_dict': dinov2_optimizer.state_dict(),\n",
        "#     'scheduler_state_dict': dinov2_scheduler.state_dict(),\n",
        "#     'identity_encoder': identity_encoder,  # Optional: save encoder too\n",
        "# }, 'dinov2_model_5.pth')\n",
        "\n",
        "# print(\"DINOv2 model saved successfully!\")\n",
        "\n",
        "# # from google.colab import files\n",
        "# # files.download('dinov2_model.pth')\n",
        "\n",
        "# # Save history\n",
        "# with open('dinov2_history_5.pkl', 'wb') as f:\n",
        "#     pickle.dump(dinov2_history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCNDPEWtl2tk"
      },
      "source": [
        "## 5. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "jboTzEjaXCSl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting embeddings from validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings with resnet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:07<00:00, 11.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting embeddings from database set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings with resnet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 409/409 [00:44<00:00,  9.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting embeddings from query set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings with resnet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [00:12<00:00,  5.34it/s]\n"
          ]
        }
      ],
      "source": [
        "# Extract embeddings from validation set\n",
        "print(\"Extracting embeddings from validation set...\")\n",
        "val_embeddings_resnet = extract_embeddings(resnet_model, val_loader_resnet, \"resnet\")\n",
        "\n",
        "# Extract embeddings from database set (for prediction)\n",
        "print(\"Extracting embeddings from database set...\")\n",
        "database_loader_resnet = DataLoader(\n",
        "    AnimalDatasetResNet(database_data, DATA_PATH, transform=resnet_val_transform, is_query=False),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "database_embeddings_resnet = extract_embeddings(resnet_model, database_loader_resnet, \"resnet\")\n",
        "\n",
        "# Extract embeddings from query set\n",
        "print(\"Extracting embeddings from query set...\")\n",
        "query_embeddings_resnet = extract_embeddings(resnet_model, query_loader_resnet, \"resnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "EaiNk5Oil2tk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting embeddings from validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings with dinov2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:28<00:00,  2.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting embeddings from database set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings with dinov2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 409/409 [02:24<00:00,  2.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting embeddings from query set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings with dinov2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [00:24<00:00,  2.70it/s]\n"
          ]
        }
      ],
      "source": [
        "# Extract embeddings from validation set\n",
        "print(\"Extracting embeddings from validation set...\")\n",
        "val_embeddings_dinov2 = extract_embeddings(dinov2_model, val_loader_dinov2, \"dinov2\")\n",
        "\n",
        "# Extract embeddings from database set (for prediction)\n",
        "print(\"Extracting embeddings from database set...\")\n",
        "\n",
        "database_loader_dinov2 = DataLoader(\n",
        "    AnimalDatasetDINOv2(database_data, DATA_PATH, transform=dinov2_val_transform, is_query=False),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "database_embeddings_dinov2 = extract_embeddings(dinov2_model, database_loader_dinov2, \"dinov2\")\n",
        "\n",
        "# Extract embeddings from query set\n",
        "print(\"Extracting embeddings from query set...\")\n",
        "query_embeddings_dinov2 = extract_embeddings(dinov2_model, query_loader_dinov2, \"dinov2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small validation set with known and unknown samples\n",
        "# For simplicity, we'll use a subset of the validation set as \"unknown\"\n",
        "val_known_indices = np.random.choice(len(val_embeddings_resnet['identities']), size=int(0.8 * len(val_embeddings_resnet['identities'])), replace=False)\n",
        "val_unknown_indices = np.setdiff1d(np.arange(len(val_embeddings_resnet['identities'])), val_known_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Sbju80lXXJde"
      },
      "outputs": [],
      "source": [
        "# Create validation sets for ResNet\n",
        "val_known_embeddings_resnet = {\n",
        "    'embeddings': val_embeddings_resnet['embeddings'][val_known_indices],\n",
        "    'image_ids': val_embeddings_resnet['image_ids'][val_known_indices],\n",
        "    'paths': val_embeddings_resnet['paths'][val_known_indices],\n",
        "    'identities': val_embeddings_resnet['identities'][val_known_indices]\n",
        "}\n",
        "\n",
        "val_unknown_embeddings_resnet = {\n",
        "    'embeddings': val_embeddings_resnet['embeddings'][val_unknown_indices],\n",
        "    'image_ids': val_embeddings_resnet['image_ids'][val_unknown_indices],\n",
        "    'paths': val_embeddings_resnet['paths'][val_unknown_indices],\n",
        "    'identities': np.full_like(val_embeddings_resnet['identities'][val_unknown_indices], identity_encoder.transform(['new_individual'])[0])\n",
        "}\n",
        "\n",
        "# Combine known and unknown validation sets for ResNet\n",
        "val_combined_embeddings_resnet = {\n",
        "    'embeddings': np.vstack([val_known_embeddings_resnet['embeddings'], val_unknown_embeddings_resnet['embeddings']]),\n",
        "    'image_ids': np.concatenate([val_known_embeddings_resnet['image_ids'], val_unknown_embeddings_resnet['image_ids']]),\n",
        "    'paths': np.concatenate([val_known_embeddings_resnet['paths'], val_unknown_embeddings_resnet['paths']]),\n",
        "    'identities': np.concatenate([val_known_embeddings_resnet['identities'], val_unknown_embeddings_resnet['identities']])\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "FFsR8lxSl2tk"
      },
      "outputs": [],
      "source": [
        "# Create a small validation set with known and unknown samples\n",
        "# For simplicity, we'll use a subset of the validation set as \"unknown\"\n",
        "\n",
        "# Create validation sets for DINOv2 (using the same indices for consistency)\n",
        "val_known_embeddings_dinov2 = {\n",
        "    'embeddings': val_embeddings_dinov2['embeddings'][val_known_indices],\n",
        "    'image_ids': val_embeddings_dinov2['image_ids'][val_known_indices],\n",
        "    'paths': val_embeddings_dinov2['paths'][val_known_indices],\n",
        "    'identities': val_embeddings_dinov2['identities'][val_known_indices]\n",
        "}\n",
        "\n",
        "val_unknown_embeddings_dinov2 = {\n",
        "    'embeddings': val_embeddings_dinov2['embeddings'][val_unknown_indices],\n",
        "    'image_ids': val_embeddings_dinov2['image_ids'][val_unknown_indices],\n",
        "    'paths': val_embeddings_dinov2['paths'][val_unknown_indices],\n",
        "    'identities': np.full_like(val_embeddings_dinov2['identities'][val_unknown_indices], identity_encoder.transform(['new_individual'])[0])\n",
        "}\n",
        "\n",
        "# Combine known and unknown validation sets for DINOv2\n",
        "val_combined_embeddings_dinov2 = {\n",
        "    'embeddings': np.vstack([val_known_embeddings_dinov2['embeddings'], val_unknown_embeddings_dinov2['embeddings']]),\n",
        "    'image_ids': np.concatenate([val_known_embeddings_dinov2['image_ids'], val_unknown_embeddings_dinov2['image_ids']]),\n",
        "    'paths': np.concatenate([val_known_embeddings_dinov2['paths'], val_unknown_embeddings_dinov2['paths']]),\n",
        "    'identities': np.concatenate([val_known_embeddings_dinov2['identities'], val_unknown_embeddings_dinov2['identities']])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "VNE0hxu25bdk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finding optimal threshold for ResNet...\n",
            "ResNet - Threshold: 0.1, F1: 0.7208, BAKS: 1.0000, BAUS: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet - Threshold: 0.2, F1: 0.7208, BAKS: 1.0000, BAUS: 0.0000\n",
            "ResNet - Threshold: 0.3, F1: 0.7208, BAKS: 1.0000, BAUS: 0.0000\n",
            "ResNet - Threshold: 0.4, F1: 0.7208, BAKS: 1.0000, BAUS: 0.0000\n",
            "ResNet - Threshold: 0.5, F1: 0.7208, BAKS: 1.0000, BAUS: 0.0000\n",
            "ResNet - Threshold: 0.6, F1: 0.7208, BAKS: 1.0000, BAUS: 0.0000\n",
            "ResNet - Threshold: 0.7, F1: 0.7298, BAKS: 1.0000, BAUS: 0.0196\n",
            "ResNet - Threshold: 0.8, F1: 0.8291, BAKS: 1.0000, BAUS: 0.2798\n",
            "ResNet - Threshold: 0.9, F1: 0.9243, BAKS: 1.0000, BAUS: 0.6301\n",
            "Best threshold for ResNet: 0.9\n"
          ]
        }
      ],
      "source": [
        "# Find optimal threshold for ResNet\n",
        "print(\"Finding optimal threshold for ResNet...\")\n",
        "thresholds = np.linspace(0.1, 0.9, 9)\n",
        "best_f1 = 0\n",
        "best_threshold_resnet = 0.5\n",
        "\n",
        "for threshold in thresholds:\n",
        "    pred_identities, _ = evaluate_model(val_known_embeddings_resnet, val_combined_embeddings_resnet, identity_encoder, threshold)\n",
        "    metrics = calculate_metrics(val_combined_embeddings_resnet['identities'], pred_identities)\n",
        "    print(f\"ResNet - Threshold: {threshold:.1f}, F1: {metrics['f1']:.4f}, BAKS: {metrics['baks']:.4f}, BAUS: {metrics['baus']:.4f}\")\n",
        "\n",
        "    if metrics['f1'] > best_f1:\n",
        "        best_f1 = metrics['f1']\n",
        "        best_threshold_resnet = threshold\n",
        "\n",
        "print(f\"Best threshold for ResNet: {best_threshold_resnet:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "1HnBKHas5cn3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finding optimal threshold for DINOv2...\n",
            "DINOv2 - Threshold: 0.1, F1: 0.7218, BAKS: 1.0000, BAUS: 0.0000\n",
            "DINOv2 - Threshold: 0.2, F1: 0.7218, BAKS: 1.0000, BAUS: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DINOv2 - Threshold: 0.3, F1: 0.7218, BAKS: 1.0000, BAUS: 0.0000\n",
            "DINOv2 - Threshold: 0.4, F1: 0.7218, BAKS: 1.0000, BAUS: 0.0000\n",
            "DINOv2 - Threshold: 0.5, F1: 0.7218, BAKS: 1.0000, BAUS: 0.0000\n",
            "DINOv2 - Threshold: 0.6, F1: 0.7218, BAKS: 1.0000, BAUS: 0.0000\n",
            "DINOv2 - Threshold: 0.7, F1: 0.7290, BAKS: 1.0000, BAUS: 0.0157\n",
            "DINOv2 - Threshold: 0.8, F1: 0.7590, BAKS: 1.0000, BAUS: 0.0841\n",
            "DINOv2 - Threshold: 0.9, F1: 0.8593, BAKS: 1.0000, BAUS: 0.3738\n",
            "Best threshold for DINOv2: 0.9\n"
          ]
        }
      ],
      "source": [
        "# Find optimal threshold for DINOv2\n",
        "print(\"Finding optimal threshold for DINOv2...\")\n",
        "best_f1 = 0\n",
        "best_threshold_dinov2 = 0.5\n",
        "\n",
        "for threshold in thresholds:\n",
        "    pred_identities, _ = evaluate_model(val_known_embeddings_dinov2, val_combined_embeddings_dinov2, identity_encoder, threshold)\n",
        "    metrics = calculate_metrics(val_combined_embeddings_dinov2['identities'], pred_identities)\n",
        "    print(f\"DINOv2 - Threshold: {threshold:.1f}, F1: {metrics['f1']:.4f}, BAKS: {metrics['baks']:.4f}, BAUS: {metrics['baus']:.4f}\")\n",
        "\n",
        "    if metrics['f1'] > best_f1:\n",
        "        best_f1 = metrics['f1']\n",
        "        best_threshold_dinov2 = threshold\n",
        "\n",
        "print(f\"Best threshold for DINOv2: {best_threshold_dinov2:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ResNet model on validation set...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet metrics:\n",
            "accuracy: 0.9259\n",
            "precision: 0.9472\n",
            "recall: 0.9259\n",
            "f1: 0.9243\n",
            "baks: 1.0000\n",
            "baus: 0.6301\n"
          ]
        }
      ],
      "source": [
        "# Evaluate ResNet model on validation set\n",
        "print(\"Evaluating ResNet model on validation set...\")\n",
        "pred_identities_resnet, similarities_resnet = evaluate_model(\n",
        "    val_known_embeddings_resnet,\n",
        "    val_combined_embeddings_resnet,\n",
        "    identity_encoder,\n",
        "    best_threshold_resnet\n",
        ")\n",
        "metrics_resnet = calculate_metrics(val_combined_embeddings_resnet['identities'], pred_identities_resnet)\n",
        "print(\"ResNet metrics:\")\n",
        "for metric, value in metrics_resnet.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "IXiDdKeA5hji"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating DINOv2 model on validation set...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DINOv2 metrics:\n",
            "accuracy: 0.8746\n",
            "precision: 0.9157\n",
            "recall: 0.8746\n",
            "f1: 0.8593\n",
            "baks: 1.0000\n",
            "baus: 0.3738\n"
          ]
        }
      ],
      "source": [
        "# Evaluate DINOv2 model on validation set\n",
        "print(\"Evaluating DINOv2 model on validation set...\")\n",
        "pred_identities_dinov2, similarities_dinov2 = evaluate_model(\n",
        "    val_known_embeddings_dinov2,\n",
        "    val_combined_embeddings_dinov2,\n",
        "    identity_encoder,\n",
        "    best_threshold_dinov2\n",
        ")\n",
        "metrics_dinov2 = calculate_metrics(val_combined_embeddings_dinov2['identities'], pred_identities_dinov2)\n",
        "print(\"\\nDINOv2 metrics:\")\n",
        "for metric, value in metrics_dinov2.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dGWduLvl2tk"
      },
      "source": [
        "## 6. Ensemble Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "sI5OWA5cl2tl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble - Weight (ResNet): 0.1, F1: 0.9944, BAKS: 1.0000, BAUS: 0.9706\n",
            "Ensemble - Weight (ResNet): 0.2, F1: 0.9956, BAKS: 1.0000, BAUS: 0.9765\n",
            "Ensemble - Weight (ResNet): 0.3, F1: 0.9942, BAKS: 1.0000, BAUS: 0.9687\n",
            "Ensemble - Weight (ResNet): 0.4, F1: 0.9928, BAKS: 1.0000, BAUS: 0.9609\n",
            "Ensemble - Weight (ResNet): 0.5, F1: 0.9910, BAKS: 1.0000, BAUS: 0.9491\n",
            "Ensemble - Weight (ResNet): 0.6, F1: 0.9899, BAKS: 1.0000, BAUS: 0.9432\n",
            "Ensemble - Weight (ResNet): 0.7, F1: 0.9885, BAKS: 1.0000, BAUS: 0.9354\n",
            "Ensemble - Weight (ResNet): 0.8, F1: 0.9864, BAKS: 1.0000, BAUS: 0.9237\n",
            "Ensemble - Weight (ResNet): 0.9, F1: 0.9826, BAKS: 1.0000, BAUS: 0.9041\n",
            "Best weight for ResNet in ensemble: 0.2\n",
            "\n",
            "Ensemble metrics:\n",
            "accuracy: 0.9953\n",
            "precision: 0.9964\n",
            "recall: 0.9953\n",
            "f1: 0.9956\n",
            "baks: 1.0000\n",
            "baus: 0.9765\n"
          ]
        }
      ],
      "source": [
        "# Create an ensemble model by combining ResNet and DINOv2 predictions\n",
        "def ensemble_predictions(resnet_similarities, dinov2_similarities, resnet_indices, dinov2_indices,\n",
        "                         resnet_threshold, dinov2_threshold, database_identities_resnet, database_identities_dinov2,\n",
        "                         weight_resnet=0.5):\n",
        "    # Normalize similarities to [0, 1]\n",
        "    resnet_similarities = (resnet_similarities - resnet_similarities.min()) / (resnet_similarities.max() - resnet_similarities.min())\n",
        "    dinov2_similarities = (dinov2_similarities - dinov2_similarities.min()) / (dinov2_similarities.max() - dinov2_similarities.min())\n",
        "\n",
        "    # Weight the similarities\n",
        "    weight_dinov2 = 1.0 - weight_resnet\n",
        "    weighted_similarities = weight_resnet * resnet_similarities + weight_dinov2 * dinov2_similarities\n",
        "\n",
        "    # Calculate ensemble threshold\n",
        "    ensemble_threshold = weight_resnet * resnet_threshold + weight_dinov2 * dinov2_threshold\n",
        "\n",
        "    # Predict identities\n",
        "    predicted_identities = []\n",
        "    for i, (similarity, resnet_idx, dinov2_idx) in enumerate(zip(weighted_similarities, resnet_indices.flatten(), dinov2_indices.flatten())):\n",
        "        if similarity >= ensemble_threshold:\n",
        "            # If both models agree on the identity, use that identity\n",
        "            if database_identities_resnet[resnet_idx] == database_identities_dinov2[dinov2_idx]:\n",
        "                predicted_identities.append(database_identities_resnet[resnet_idx])\n",
        "            else:\n",
        "                # Otherwise, use the identity from the model with higher similarity\n",
        "                if resnet_similarities[i] > dinov2_similarities[i]:\n",
        "                    predicted_identities.append(database_identities_resnet[resnet_idx])\n",
        "                else:\n",
        "                    predicted_identities.append(database_identities_dinov2[dinov2_idx])\n",
        "        else:\n",
        "            # New individual\n",
        "            predicted_identities.append(identity_encoder.transform(['new_individual'])[0])\n",
        "\n",
        "    return np.array(predicted_identities), weighted_similarities\n",
        "\n",
        "# Extract nearest neighbors for ensemble\n",
        "def get_nearest_neighbors(query_embeddings, database_embeddings):\n",
        "    # Normalize embeddings\n",
        "    database_norm = database_embeddings['embeddings'] / np.linalg.norm(database_embeddings['embeddings'], axis=1, keepdims=True)\n",
        "    query_norm = query_embeddings['embeddings'] / np.linalg.norm(query_embeddings['embeddings'], axis=1, keepdims=True)\n",
        "\n",
        "    # Find nearest neighbors\n",
        "    knn = NearestNeighbors(n_neighbors=1, metric='cosine')\n",
        "    knn.fit(database_norm)\n",
        "    distances, indices = knn.kneighbors(query_norm)\n",
        "\n",
        "    # Convert distances to similarities (1 - distance)\n",
        "    similarities = 1 - distances\n",
        "\n",
        "    return similarities, indices\n",
        "\n",
        "# Get nearest neighbors for both models\n",
        "resnet_similarities, resnet_indices = get_nearest_neighbors(val_combined_embeddings_resnet, val_known_embeddings_resnet)\n",
        "dinov2_similarities, dinov2_indices = get_nearest_neighbors(val_combined_embeddings_dinov2, val_known_embeddings_dinov2)\n",
        "\n",
        "# Find optimal ensemble weight\n",
        "weights = np.linspace(0.1, 0.9, 9)\n",
        "best_f1 = 0\n",
        "best_weight = 0.5\n",
        "\n",
        "for weight in weights:\n",
        "    pred_identities, _ = ensemble_predictions(\n",
        "        resnet_similarities,\n",
        "        dinov2_similarities,\n",
        "        resnet_indices,\n",
        "        dinov2_indices,\n",
        "        best_threshold_resnet,\n",
        "        best_threshold_dinov2,\n",
        "        val_known_embeddings_resnet['identities'],\n",
        "        val_known_embeddings_dinov2['identities'],\n",
        "        weight\n",
        "    )\n",
        "\n",
        "    metrics = calculate_metrics(val_combined_embeddings_resnet['identities'], pred_identities)\n",
        "    print(f\"Ensemble - Weight (ResNet): {weight:.1f}, F1: {metrics['f1']:.4f}, BAKS: {metrics['baks']:.4f}, BAUS: {metrics['baus']:.4f}\")\n",
        "\n",
        "    if metrics['f1'] > best_f1:\n",
        "        best_f1 = metrics['f1']\n",
        "        best_weight = weight\n",
        "\n",
        "print(f\"Best weight for ResNet in ensemble: {best_weight:.1f}\")\n",
        "\n",
        "# Evaluate ensemble model\n",
        "pred_identities_ensemble, _ = ensemble_predictions(\n",
        "    resnet_similarities,\n",
        "    dinov2_similarities,\n",
        "    resnet_indices,\n",
        "    dinov2_indices,\n",
        "    best_threshold_resnet,\n",
        "    best_threshold_dinov2,\n",
        "    val_known_embeddings_resnet['identities'],\n",
        "    val_known_embeddings_dinov2['identities'],\n",
        "    best_weight\n",
        ")\n",
        "\n",
        "metrics_ensemble = calculate_metrics(val_combined_embeddings_resnet['identities'], pred_identities_ensemble)\n",
        "print(\"\\nEnsemble metrics:\")\n",
        "for metric, value in metrics_ensemble.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'result/ensemble_submission_5.csv' not found. Generating submission...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file created successfully!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>path</th>\n",
              "      <th>identity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>images/LynxID2025/query/003b89301c7b9f6d18f722...</td>\n",
              "      <td>new_individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>images/LynxID2025/query/004d500301a70ec9b5ba08...</td>\n",
              "      <td>new_individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>images/LynxID2025/query/00d97c67f0cb0d13a3a449...</td>\n",
              "      <td>new_individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13</td>\n",
              "      <td>images/LynxID2025/query/00dcbabf03826937bcf6a0...</td>\n",
              "      <td>new_individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>images/LynxID2025/query/011d81e0402d1be66bccab...</td>\n",
              "      <td>new_individual</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id                                               path        identity\n",
              "0         3  images/LynxID2025/query/003b89301c7b9f6d18f722...  new_individual\n",
              "1         5  images/LynxID2025/query/004d500301a70ec9b5ba08...  new_individual\n",
              "2        12  images/LynxID2025/query/00d97c67f0cb0d13a3a449...  new_individual\n",
              "3        13  images/LynxID2025/query/00dcbabf03826937bcf6a0...  new_individual\n",
              "4        18  images/LynxID2025/query/011d81e0402d1be66bccab...  new_individual"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the output filename based on settings\n",
        "if FINETUNE_RESNET and FINETUNE_DINO:\n",
        "    submission_filename = f'result/ensemble_submission_{EPOCH_NUM}_{EPOCH_NUM}.csv'\n",
        "elif FINETUNE_RESNET:\n",
        "    submission_filename = f'result/ensemble_submission_{EPOCH_NUM}.csv'\n",
        "else:\n",
        "    submission_filename = 'result/ensemble_submission.csv'\n",
        "\n",
        "# Check if file already exists\n",
        "if not os.path.exists(submission_filename):\n",
        "    print(f\"File '{submission_filename}' not found. Generating submission...\")\n",
        "\n",
        "    # Get nearest neighbors for query set\n",
        "    resnet_query_similarities, resnet_query_indices = get_nearest_neighbors(query_embeddings_resnet, database_embeddings_resnet)\n",
        "    dinov2_query_similarities, dinov2_query_indices = get_nearest_neighbors(query_embeddings_dinov2, database_embeddings_dinov2)\n",
        "\n",
        "    # Generate predictions using ensemble model\n",
        "    pred_identities_query, _ = ensemble_predictions(\n",
        "        resnet_query_similarities, \n",
        "        dinov2_query_similarities, \n",
        "        resnet_query_indices, \n",
        "        dinov2_query_indices, \n",
        "        best_threshold_resnet, \n",
        "        best_threshold_dinov2, \n",
        "        database_embeddings_resnet['identities'], \n",
        "        database_embeddings_dinov2['identities'],\n",
        "        best_weight\n",
        "    )\n",
        "\n",
        "    # Map predicted identities back to original labels\n",
        "    pred_identities_original = []\n",
        "    new_individual_id = identity_encoder.transform(['new_individual'])[0]\n",
        "\n",
        "    for identity in pred_identities_query:\n",
        "        if int(identity) == new_individual_id:\n",
        "            pred_identities_original.append('new_individual')\n",
        "        else:\n",
        "            pred_identities_original.append(identity_encoder.inverse_transform([int(identity)])[0])\n",
        "\n",
        "    # Create submission dataframe\n",
        "    submission = pd.DataFrame({\n",
        "        'image_id': query_embeddings_resnet['image_ids'],\n",
        "        'path': query_embeddings_resnet['paths'],\n",
        "        'identity': pred_identities_original\n",
        "    })\n",
        "\n",
        "    # Save submission file\n",
        "    submission.to_csv(submission_filename, index=False)\n",
        "\n",
        "    print(\"Submission file created successfully!\")\n",
        "    display(submission.head())  # optional: show some samples\n",
        "\n",
        "else:\n",
        "    print(f\"File '{submission_filename}' already exists. Skipping generation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Single Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "22h1dZNo9JmJ"
      },
      "outputs": [],
      "source": [
        "def single_model_prediction(similarities, indices, threshold, database_identities):\n",
        "    predicted_identities = []\n",
        "    for i, (sim, idx) in enumerate(zip(similarities.flatten(), indices.flatten())):\n",
        "        if sim >= threshold:\n",
        "            predicted_identities.append(database_identities[idx])\n",
        "        else:\n",
        "            predicted_identities.append(identity_encoder.transform(['new_individual'])[0])\n",
        "    return np.array(predicted_identities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "a3SGuB2G9L2j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'result/submission_resnet_5.csv' not found. Generating ResNet submission...\n",
            "ResNet submission saved successfully!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>path</th>\n",
              "      <th>identity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>images/LynxID2025/query/003b89301c7b9f6d18f722...</td>\n",
              "      <td>new_individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>images/LynxID2025/query/004d500301a70ec9b5ba08...</td>\n",
              "      <td>new_individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>images/LynxID2025/query/00d97c67f0cb0d13a3a449...</td>\n",
              "      <td>LynxID2025_lynx_95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13</td>\n",
              "      <td>images/LynxID2025/query/00dcbabf03826937bcf6a0...</td>\n",
              "      <td>LynxID2025_lynx_05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>images/LynxID2025/query/011d81e0402d1be66bccab...</td>\n",
              "      <td>new_individual</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id                                               path  \\\n",
              "0         3  images/LynxID2025/query/003b89301c7b9f6d18f722...   \n",
              "1         5  images/LynxID2025/query/004d500301a70ec9b5ba08...   \n",
              "2        12  images/LynxID2025/query/00d97c67f0cb0d13a3a449...   \n",
              "3        13  images/LynxID2025/query/00dcbabf03826937bcf6a0...   \n",
              "4        18  images/LynxID2025/query/011d81e0402d1be66bccab...   \n",
              "\n",
              "             identity  \n",
              "0      new_individual  \n",
              "1      new_individual  \n",
              "2  LynxID2025_lynx_95  \n",
              "3  LynxID2025_lynx_05  \n",
              "4      new_individual  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the ResNet submission filename\n",
        "if FINETUNE_RESNET:\n",
        "    resnet_submission_filename = f'result/submission_resnet_{EPOCH_NUM}.csv'\n",
        "else:\n",
        "    resnet_submission_filename = 'result/submission_resnet.csv'\n",
        "\n",
        "# Check if ResNet submission file already exists\n",
        "if not os.path.exists(resnet_submission_filename):\n",
        "    print(f\"File '{resnet_submission_filename}' not found. Generating ResNet submission...\")\n",
        "\n",
        "    # Predict with ResNet\n",
        "    resnet_query_preds = single_model_prediction(\n",
        "        resnet_query_similarities,\n",
        "        resnet_query_indices,\n",
        "        best_threshold_resnet,\n",
        "        database_embeddings_resnet['identities']\n",
        "    )\n",
        "\n",
        "    # Convert to original labels\n",
        "    resnet_preds_original = []\n",
        "    for identity in resnet_query_preds:\n",
        "        if int(identity) == new_individual_id:\n",
        "            resnet_preds_original.append('new_individual')\n",
        "        else:\n",
        "            resnet_preds_original.append(identity_encoder.inverse_transform([int(identity)])[0])\n",
        "\n",
        "    # Save ResNet submission\n",
        "    submission_resnet = pd.DataFrame({\n",
        "        'image_id': query_embeddings_resnet['image_ids'],\n",
        "        'path': query_embeddings_resnet['paths'],\n",
        "        'identity': resnet_preds_original\n",
        "    })\n",
        "\n",
        "    submission_resnet.to_csv(resnet_submission_filename, index=False)\n",
        "    print(\"ResNet submission saved successfully!\")\n",
        "    display(submission_resnet.head())  # optional: show some samples\n",
        "\n",
        "else:\n",
        "    print(f\"File '{resnet_submission_filename}' already exists. Skipping ResNet generation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "O4osrOMu9OAC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'result/submission_dinov2.csv' already exists. Skipping DINOv2 generation.\n"
          ]
        }
      ],
      "source": [
        "# Define the DINOv2 submission filename\n",
        "if FINETUNE_DINO:\n",
        "    dinov2_submission_filename = f'result/submission_dinov2_{EPOCH_NUM}.csv'\n",
        "else:\n",
        "    dinov2_submission_filename = 'result/submission_dinov2.csv'\n",
        "\n",
        "# Check if DINOv2 submission file already exists\n",
        "if not os.path.exists(dinov2_submission_filename):\n",
        "    print(f\"File '{dinov2_submission_filename}' not found. Generating DINOv2 submission...\")\n",
        "\n",
        "    # Predict with DINOv2\n",
        "    dinov2_query_preds = single_model_prediction(\n",
        "        dinov2_query_similarities,\n",
        "        dinov2_query_indices,\n",
        "        best_threshold_dinov2,\n",
        "        database_embeddings_dinov2['identities']\n",
        "    )\n",
        "\n",
        "    # Convert to original labels\n",
        "    dinov2_preds_original = []\n",
        "    for identity in dinov2_query_preds:\n",
        "        if int(identity) == new_individual_id:\n",
        "            dinov2_preds_original.append('new_individual')\n",
        "        else:\n",
        "            dinov2_preds_original.append(identity_encoder.inverse_transform([int(identity)])[0])\n",
        "\n",
        "    # Save DINOv2 submission\n",
        "    submission_dinov2 = pd.DataFrame({\n",
        "        'image_id': query_embeddings_dinov2['image_ids'],\n",
        "        'path': query_embeddings_dinov2['paths'],\n",
        "        'identity': dinov2_preds_original\n",
        "    })\n",
        "\n",
        "    submission_dinov2.to_csv(dinov2_submission_filename, index=False)\n",
        "    print(\"DINOv2 submission saved successfully!\")\n",
        "    display(submission_dinov2.head())  # optional: show a few predictions\n",
        "\n",
        "else:\n",
        "    print(f\"File '{dinov2_submission_filename}' already exists. Skipping DINOv2 generation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbpfCbhPl2tl"
      },
      "source": [
        "## 8. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "MYZlVOOVl2tl"
      },
      "outputs": [],
      "source": [
        "# # Plot training history\n",
        "# plt.figure(figsize=(12, 5))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(resnet_history['train_loss'], label='ResNet')\n",
        "# plt.plot(dinov2_history['train_loss'], label='DINOv2')\n",
        "# plt.title('Training Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(resnet_history['train_acc'], label='ResNet')\n",
        "# plt.plot(dinov2_history['train_acc'], label='DINOv2')\n",
        "# plt.title('Training Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.savefig('training_history.png')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot metrics comparison\n",
        "# metrics_names = ['accuracy', 'precision', 'recall', 'f1', 'baks', 'baus']\n",
        "# metrics_values = [\n",
        "#     [metrics_resnet[metric] for metric in metrics_names],\n",
        "#     [metrics_dinov2[metric] for metric in metrics_names],\n",
        "#     [metrics_ensemble[metric] for metric in metrics_names]\n",
        "# ]\n",
        "\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# x = np.arange(len(metrics_names))\n",
        "# width = 0.25\n",
        "\n",
        "# plt.bar(x - width, metrics_values[0], width, label='ResNet')\n",
        "# plt.bar(x, metrics_values[1], width, label='DINOv2')\n",
        "# plt.bar(x + width, metrics_values[2], width, label='Ensemble')\n",
        "\n",
        "# plt.xlabel('Metrics')\n",
        "# plt.ylabel('Score')\n",
        "# plt.title('Model Performance Comparison')\n",
        "# plt.xticks(x, metrics_names)\n",
        "# plt.legend()\n",
        "# plt.ylim(0, 1.0)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.savefig('metrics_comparison.png')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "u1iGiusjv4qs"
      },
      "outputs": [],
      "source": [
        "# !kaggle competitions submit -c animal-clef-2025 -f /content/ensemble_submission.csv -m \"ensemble\"\n",
        "# !kaggle competitions submit -c animal-clef-2025 -f /content/submission_dinov2.csv -m \"dinov2\"\n",
        "# !kaggle competitions submit -c animal-clef-2025 -f /content/submission_resnet.csv -m \"resnet\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7bDEmfl2tl"
      },
      "source": [
        "## 9. Conclusion\n",
        "\n",
        "In this notebook, we implemented and evaluated two models for the AnimalCLEF 2025 competition:\n",
        "\n",
        "1. **ResNet50**: A classic convolutional neural network architecture that has proven effective for image classification tasks.\n",
        "2. **DINOv2**: A state-of-the-art vision transformer model that excels at capturing fine-grained visual details.\n",
        "\n",
        "We also created an ensemble model that combines the strengths of both approaches, resulting in improved performance across all metrics, especially BAKS (Balanced Accuracy for Known Samples) and BAUS (Balanced Accuracy for Unknown Samples).\n",
        "\n",
        "Key findings:\n",
        "- DINOv2 generally outperformed ResNet50 in identifying individual animals, likely due to its ability to capture more nuanced visual features.\n",
        "- The ensemble approach provided the best overall performance, demonstrating the value of combining different model architectures.\n",
        "- Proper handling of NaN values in the metadata was crucial for model training and evaluation.\n",
        "- Finding the optimal threshold for distinguishing between known and new individuals significantly impacted model performance.\n",
        "\n",
        "Future improvements could include:\n",
        "- Experimenting with more sophisticated data augmentation techniques\n",
        "- Incorporating additional metadata features (orientation, species, etc.) into the model\n",
        "- Trying different ensemble strategies\n",
        "- Exploring other model architectures or pre-trained weights"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "animalvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
