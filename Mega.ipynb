{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7db823",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio timm numpy scikit-learn matplotlib pandas wildlife-datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c64432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import AutoModel\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle or Colab\n",
    "IN_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(\"Running in Kaggle\")\n",
    "    # Set data path\n",
    "    DATA_PATH = '/kaggle/input/animal-clef-2025'\n",
    "elif IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Mount Google Drive if in Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Install required packages\n",
    "    !pip install -q git+https://github.com/WildlifeDatasets/wildlife-datasets@develop\n",
    "    !pip install -q git+https://github.com/WildlifeDatasets/wildlife-tools\n",
    "    !pip install -q timm transformers\n",
    "# Set data path\n",
    "    DATA_PATH = '/content/drive/MyDrive/5524_CVEVAN/animal-clef-2025'\n",
    "else:\n",
    "    print(\"Running in local environment\")\n",
    "    # Set appropriate path for your environment\n",
    "    DATA_PATH = './animal-clef-2025'\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635583b",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14174643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(os.path.join(DATA_PATH, 'metadata.csv'))\n",
    "print(f\"Metadata shape: {metadata.shape}\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19aaf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Explore dataset statistics\n",
    "print(f\"Total images: {len(metadata)}\")\n",
    "print(f\"Database images: {len(metadata[metadata['split'] == 'database'])}\")\n",
    "print(f\"Query images: {len(metadata[metadata['split'] == 'query'])}\")\n",
    "\n",
    "# Count species\n",
    "species_counts = metadata['species'].value_counts()\n",
    "print(\"\\nSpecies distribution:\")\n",
    "print(species_counts)\n",
    "\n",
    "# Visualize species distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=species_counts.index, y=species_counts.values)\n",
    "plt.title('Species Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count unique individuals\n",
    "unique_individuals = metadata[metadata['identity'].notna()]['identity'].nunique()\n",
    "print(f\"\\nNumber of unique individuals: {unique_individuals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "\n",
    "# Create custom dataset class\n",
    "class AnimalReIDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, metadata, root_dir, transform=None, split='database'):\n",
    "        self.metadata = metadata[metadata['split'] == split].reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        # Create label mapping for identities\n",
    "        if split in ['database', 'train']:\n",
    "            self.identities = sorted(self.metadata['identity'].dropna().unique())\n",
    "            self.identity_to_idx = {identity: idx for idx, identity in enumerate(self.identities)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row['path'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # For database images, return image, identity index, and identity\n",
    "        # if self.metadata['split'].iloc[0] == 'database':\n",
    "        if self.split in ['database', 'train']:\n",
    "            identity = row['identity']\n",
    "            if pd.isna(identity):\n",
    "                identity_idx = -1  # For images without identity\n",
    "            else:\n",
    "                identity_idx = self.identity_to_idx[identity]\n",
    "            return image, identity_idx, identity\n",
    "        # For query images, return image and image_id\n",
    "        else:\n",
    "            return image, row['image_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f4eea",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction and Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd497dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from database images\n",
    "def extract_database_embeddings(model, data_loader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    identities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, identity in tqdm(data_loader, desc='Extracting database embeddings'):\n",
    "            images = images.to(device)\n",
    "            batch_embeddings = model(images)\n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            identities.extend(identity)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings, identities\n",
    "\n",
    "# Extract embeddings from query images\n",
    "def extract_query_embeddings(model, data_loader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    image_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, ids in tqdm(data_loader, desc='Extracting query embeddings'):\n",
    "            images = images.to(device)\n",
    "            batch_embeddings = model(images)\n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            image_ids.extend(ids.numpy())\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings, image_ids\n",
    "\n",
    "# Perform retrieval using k-nearest neighbors\n",
    "def perform_retrieval(db_embeddings, query_embeddings, db_identities, query_ids, k=5, threshold=0.7):\n",
    "    # Initialize k-nearest neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=min(k, len(db_embeddings)), metric='cosine')\n",
    "    knn.fit(db_embeddings)\n",
    "\n",
    "    # Find k-nearest neighbors for each query\n",
    "    distances, indices = knn.kneighbors(query_embeddings)\n",
    "\n",
    "    # Convert distances to similarities (1 - distance)\n",
    "    similarities = 1 - distances\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = []\n",
    "    for i, (sims, idxs) in enumerate(zip(similarities, indices)):\n",
    "        query_id = query_ids[i]\n",
    "\n",
    "        # Check if the highest similarity is above threshold\n",
    "        if sims[0] > threshold:\n",
    "            # Predict the identity of the most similar database image\n",
    "            prediction = db_identities[idxs[0]]\n",
    "        else:\n",
    "            # Predict as new individual\n",
    "            prediction = 'new_individual'\n",
    "\n",
    "        predictions.append({'image_id': int(query_id), 'prediction': prediction})\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb308abc",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91de818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "def evaluate_predictions(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for the predictions\n",
    "\n",
    "    Args:\n",
    "        predictions: List of predicted identities\n",
    "        ground_truths: List of ground truth identities\n",
    "\n",
    "    Returns:\n",
    "        metrics: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Convert predictions and ground truths to binary format for metrics calculation\n",
    "    # 1 for known individual, 0 for new individual\n",
    "    y_pred_binary = [0 if p == 'new_individual' else 1 for p in predictions]\n",
    "    y_true_binary = [0 if gt == 'new_individual' else 1 for gt in ground_truths]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "    precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "\n",
    "    # Calculate identity accuracy for known individuals\n",
    "    known_indices = [i for i, gt in enumerate(ground_truths) if gt != 'new_individual']\n",
    "    if known_indices:\n",
    "        known_predictions = [predictions[i] for i in known_indices]\n",
    "        known_ground_truths = [ground_truths[i] for i in known_indices]\n",
    "        identity_accuracy = sum(p == gt for p, gt in zip(known_predictions, known_ground_truths)) / len(known_indices)\n",
    "    else:\n",
    "        identity_accuracy = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'identity_accuracy': identity_accuracy\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f026ba",
   "metadata": {},
   "source": [
    "## 6. Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b697c0",
   "metadata": {},
   "source": [
    "### 6.1 Divide into train & val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b29f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['orig_split'] = metadata['split']\n",
    "\n",
    "# divide split['database'] to train and val dataset\n",
    "database_df = metadata[metadata['split'] == 'database'].copy()\n",
    "train_df, val_df = train_test_split(database_df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "metadata.loc[train_df.index, 'split'] = 'train'\n",
    "metadata.loc[val_df.index, 'split'] = 'val'\n",
    "metadata['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Define image transformations\n",
    "# Image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# construct dataset\n",
    "db_dataset = AnimalReIDDataset(metadata, DATA_PATH, transform, split='train')\n",
    "val_dataset   = AnimalReIDDataset(metadata, DATA_PATH, transform, split='val')\n",
    "query_dataset = AnimalReIDDataset(metadata, DATA_PATH, transform, split='query')\n",
    "\n",
    "# dataLoader\n",
    "db_loader = torch.utils.data.DataLoader(db_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "query_loader = torch.utils.data.DataLoader(query_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d54c4",
   "metadata": {},
   "source": [
    "### 6.2 Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "num_classes = len(db_dataset.identities) if hasattr(db_dataset, 'identities') else 0\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52360a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"hf-hub:BVRA/MegaDescriptor-L-384\", pretrained=True)\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d9636a",
   "metadata": {},
   "source": [
    "### 6.3 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings using ResNet50\n",
    "print(\"Extracting MegaDescriptor embeddings...\")\n",
    "mega_db_embeddings, mega_db_identities = extract_database_embeddings(model, db_loader)\n",
    "mega_query_embeddings, mega_query_ids = extract_query_embeddings(model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb327d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_val_embeddings, mega_val_ids = extract_query_embeddings(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380dbaa",
   "metadata": {},
   "source": [
    "### 6.4 Weight Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06eab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save embeddings to Google Drive if in Colab\n",
    "# if IN_COLAB:\n",
    "#     from google.colab import drive\n",
    "#     import os\n",
    "\n",
    "#     drive.mount('/content/drive')\n",
    "\n",
    "#     drive_path = './Mega_Embedding_Id/'\n",
    "#     os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "#     # Save MegaDescriptor embeddings\n",
    "#     np.save(drive_path + 'mega_db_embeddings.npy', mega_db_embeddings)\n",
    "#     np.save(drive_path + 'mega_query_embeddings.npy', mega_query_embeddings)\n",
    "#     np.save(drive_path + 'mega_db_identities.npy', mega_db_identities)\n",
    "#     np.save(drive_path + 'mega_query_ids.npy', mega_query_ids)\n",
    "#     np.save(drive_path + 'mega_val_embeddings.npy', mega_val_embeddings)\n",
    "#     np.save(drive_path + 'mega_val_ids.npy', mega_val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac70060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embeddings if available\n",
    "# Uncomment this section if you want to load previously saved embeddings\n",
    "drive_path = '/Mega_Embedding_Id'\n",
    "if os.path.exists(drive_path + 'resnet_db_embeddings.npy'):\n",
    "    print(\"Loading saved embeddings from Google Drive...\")\n",
    "    mega_db_embeddings = np.load(drive_path + 'mega_db_embeddings.npy')\n",
    "    mega_query_embeddings = np.load(drive_path + 'mega_query_embeddings.npy')\n",
    "    mega_db_identities = np.load(drive_path + 'mega_db_identities.npy')\n",
    "    mega_query_ids = np.load(drive_path + 'mega_query_ids.npy')\n",
    "    mega_val_embeddings = np.load(drive_path + 'mega_val_embeddings.npy')\n",
    "    mega_val_ids = np.load(drive_path + 'mega_val_ids.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ebab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MegaDescriptor\n",
    "val_metadata = metadata[metadata[\"split\"] == \"val\"]\n",
    "query_metadata = metadata[metadata[\"split\"] == \"query\"]\n",
    "ground_truths = []\n",
    "for q_id in mega_val_ids:\n",
    "    row = val_metadata[val_metadata[\"image_id\"] == q_id].iloc[0]\n",
    "    gt = row[\"identity\"] if not pd.isna(row[\"identity\"]) else \"new_individual\"\n",
    "    ground_truths.append(gt)\n",
    "\n",
    "# print(\"Performing retrieval...\")\n",
    "val_results = []\n",
    "mega_threshold = 0.5\n",
    "predicted_identities = []\n",
    "\n",
    "db_matrix = torch.tensor(mega_db_embeddings)\n",
    "val_embed_tensor = torch.tensor(mega_val_embeddings)\n",
    "\n",
    "for q_idx in tqdm(range(len(mega_val_ids))):\n",
    "    q_id = mega_val_ids[q_idx]\n",
    "    q_embed = val_embed_tensor[q_idx].unsqueeze(0)\n",
    "\n",
    "    # cosine similarity\n",
    "    sims = F.cosine_similarity(q_embed, db_matrix, dim=1)\n",
    "\n",
    "    best_idx = torch.argmax(sims).item()\n",
    "    best_score = sims[best_idx].item()\n",
    "\n",
    "    if best_score > mega_threshold:\n",
    "        prediction = mega_db_identities[best_idx]\n",
    "    else:\n",
    "        prediction = \"new_individual\"\n",
    "\n",
    "    val_results.append([q_id, prediction])\n",
    "    predicted_identities.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Binary: known vs new\n",
    "y_true_binary = [0 if gt == \"new_individual\" else 1 for gt in ground_truths]\n",
    "y_pred_binary = [0 if pred == \"new_individual\" else 1 for pred in predicted_identities]\n",
    "\n",
    "accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "\n",
    "# === identity accuracy (only for known individuals)\n",
    "known_indices = [i for i, gt in enumerate(ground_truths) if gt != \"new_individual\"]\n",
    "if known_indices:\n",
    "    correct = sum(predicted_identities[i] == ground_truths[i] for i in known_indices)\n",
    "    identity_acc = correct / len(known_indices)\n",
    "else:\n",
    "    identity_acc = 0.0\n",
    "\n",
    "print(\"\\nðŸ“Š Evaluation Metrics:\")\n",
    "print(f\"New/Known Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"Identity Accuracy (for known): {identity_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aed5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_open_set_metrics(ground_truths, predicted_identities, verbose=True):\n",
    "    \"\"\"\n",
    "    Computes BAKS, BAUS, and final geometric mean score for open-set individual identification.\n",
    "\n",
    "    Args:\n",
    "        ground_truths: List[str], ground truth identity (use \"new_individual\" for unknowns)\n",
    "        predicted_identities: List[str], predicted identity (use \"new_individual\" for unknowns)\n",
    "        verbose: bool, whether to print results\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with BAKS, BAUS, and geometric mean score\n",
    "    \"\"\"\n",
    "    known_gt_dict = defaultdict(list)\n",
    "    unknown_gt_list = []\n",
    "\n",
    "    for gt, pred in zip(ground_truths, predicted_identities):\n",
    "        if gt == \"new_individual\":\n",
    "            unknown_gt_list.append(pred == \"new_individual\")\n",
    "        else:\n",
    "            known_gt_dict[gt].append(pred == gt)\n",
    "\n",
    "    # Compute BAKS (per-class mean accuracy)\n",
    "    per_class_accs = [sum(results) / len(results) for results in known_gt_dict.values()]\n",
    "    baks = np.mean(per_class_accs) if per_class_accs else 0.0\n",
    "\n",
    "    # Compute BAUS (accuracy on unknowns)\n",
    "    baus = sum(unknown_gt_list) / len(unknown_gt_list) if unknown_gt_list else 0.0\n",
    "\n",
    "    # Geometric mean\n",
    "    final_score = np.sqrt(baks * baus)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nðŸ“Š Official Open-Set Evaluation:\")\n",
    "        print(f\"BAKS (Balanced Accuracy on Known Samples):   {baks:.4f}\")\n",
    "        print(f\"BAUS (Balanced Accuracy on Unknown Samples): {baus:.4f}\")\n",
    "        print(f\"Final Geometric Mean Score:                  {final_score:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"BAKS\": baks,\n",
    "        \"BAUS\": baus,\n",
    "        \"FinalScore\": final_score\n",
    "    }\n",
    "metrics = compute_open_set_metrics(ground_truths, predicted_identities)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform retrieval using MegaDescriptor with optimized threshold\n",
    "print(\"Performing retrieval using MegaDescriptor...\")\n",
    "mega_threshold = 0.5  # Slightly higher threshold for better precision\n",
    "mega_predictions = perform_retrieval(\n",
    "    mega_db_embeddings, mega_query_embeddings, mega_db_identities, mega_query_ids, k=5, threshold=mega_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f52fa4e",
   "metadata": {},
   "source": [
    "## 7. Create Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4798fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MegaDescriptor submission file\n",
    "def create_submission_file(predictions, filename):\n",
    "    \"\"\"\n",
    "    Create a properly formatted submission file\n",
    "\n",
    "    Args:\n",
    "        predictions: List of prediction dictionaries with 'image_id' and 'prediction' keys\n",
    "        filename: Output filename for the submission CSV\n",
    "    \"\"\"\n",
    "    # Create submission dataframe with correct format\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df['image_id'] = [pred['image_id'] for pred in predictions]\n",
    "    submission_df['identity'] = [pred['prediction'] for pred in predictions]\n",
    "\n",
    "    # Sort by image_id to ensure consistent order\n",
    "    submission_df = submission_df.sort_values('image_id').reset_index(drop=True)\n",
    "\n",
    "    # Verify that all query images are included\n",
    "    expected_query_ids = sorted(query_metadata['image_id'].values)\n",
    "    submission_ids = sorted(submission_df['image_id'].values)\n",
    "\n",
    "    if len(expected_query_ids) != len(submission_ids) or not all(a == b for a, b in zip(expected_query_ids, submission_ids)):\n",
    "        print(f\"WARNING: Submission doesn't contain all expected query IDs!\")\n",
    "        print(f\"Expected {len(expected_query_ids)} IDs, got {len(submission_ids)}\")\n",
    "\n",
    "        # Fix by creating a new dataframe with all expected IDs\n",
    "        fixed_submission = []\n",
    "        for query_id in expected_query_ids:\n",
    "            # Find the prediction for this query_id\n",
    "            match = next((pred for pred in predictions if pred['image_id'] == query_id), None)\n",
    "            if match:\n",
    "                fixed_submission.append({\n",
    "                    'image_id': query_id,\n",
    "                    'identity': match['prediction']\n",
    "                })\n",
    "            else:\n",
    "                # If no match found, use 'new_individual' as a fallback\n",
    "                fixed_submission.append({\n",
    "                    'image_id': query_id,\n",
    "                    'identity': 'new_individual'\n",
    "                })\n",
    "\n",
    "        submission_df = pd.DataFrame(fixed_submission)\n",
    "\n",
    "    # Save submission file\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"\\nSubmission file saved to '{filename}'\")\n",
    "    print(f\"Submission shape: {submission_df.shape}\")\n",
    "    print(\"First few rows of submission:\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "# Create Mega submission\n",
    "mega_submission = create_submission_file(mega_predictions, 'result/Mega_submission.csv')\n",
    "\n",
    "# Save in Google Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    mega_submission.to_csv('/content/drive/MyDrive/5524_CVEVAN/Mega_submission.csv', index=False)\n",
    "    print(\"All submissions saved to Google Drive.\")\n",
    "\n",
    "    # Download the submission files\n",
    "    from google.colab import files\n",
    "    files.download('Mega_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
